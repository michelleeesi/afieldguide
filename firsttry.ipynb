{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent circuits hum,  \n",
      "Wisdom in the code takes formâ€”  \n",
      "Machines dream of depth.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#     model=\"gpt-4o\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": \"write a haiku about ai\"}\n",
    "#     ]\n",
    "# );\n",
    "# print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. want a list of professors.csv\n",
    "# 2. update professor.csv with the work they do\n",
    "# 3. for each professor, google their social interests\n",
    "# 4 update professor.csv with their social interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serach(query: str) -> List[str]:\n",
    "    search_url = \"https://html.duckduckgo.com/html/\"\n",
    "    params = {\n",
    "        'q': query\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(search_url, data=params, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during DuckDuckGo search for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for result in soup.find_all('a', {'class': 'result__a'}, href=True):\n",
    "        links.append(result['href'])\n",
    "        if len(links) >= 5:\n",
    "            break\n",
    "    return links\n",
    "\n",
    "def extract_text(url: str, max_chars: int = 10000) -> str:\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error occurred: {e}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "        \n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "    return text[:max_chars]\n",
    "\n",
    "def get_text(querey: str, max_chars: int = 10000) -> (str, List[str]):\n",
    "    urls = serach(querey)\n",
    "    texts = []\n",
    "    for url in urls:\n",
    "        text = extract_text(url, max_chars)\n",
    "        if text:\n",
    "            texts.append(text)\n",
    "        time.sleep(1)  # Respectful delay between requests\n",
    "    combined_text = \"\\n\\n\".join(texts)\n",
    "    return combined_text, urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTResponseProf(BaseModel):\n",
    "    name: str\n",
    "    institution: str\n",
    "    department: str\n",
    "    research_areas: List[str]\n",
    "    \n",
    "def gpt_request(query: str) -> GPTResponseProf:\n",
    "    prompt = f\"Based on a given university faculty website, fill out the name, institution, department, and the main research areas of the professor: \\n\\n{query}\\n\\n Find the following name: \\n institution: \\n department: \\n research_areas: \\n\\n\\n Please provide the output in JSON format ensuring that the types are as follows: name: str     institution: str     department: str     research_areas: List[str] \" \n",
    "    response = openai.beta.chat.completions.parse(\n",
    "        model = \"gpt-4o-mini\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that looks at a url and finds information about a professor.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=GPTResponseProf\n",
    "    )\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(serach(\"mit eecs faculty\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
